{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import talib as tb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './g-research-crypto-forecasting/'\n",
    "asset_list = pd.read_csv(path+\"asset_details.csv\")\n",
    "train_dataset = pd.read_csv(path+\"train.csv\")\n",
    "validation_dataset = pd.read_csv(path+\"supplemental_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purging to avoid leakage of information\n",
    "train_dataset = train_dataset.iloc[:round(len(train_dataset)*0.95)]\n",
    "\n",
    "val_len = len(validation_dataset)\n",
    "test_dataset = validation_dataset.iloc[-round(0.45*val_len):]\n",
    "validation_dataset = validation_dataset.iloc[:round(0.45*val_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gap between train dataset and validation dataset: 60.12 days\n",
      "gap between validation dataset and test_dataset: 12.49 days\n"
     ]
    }
   ],
   "source": [
    "test_val_gap = (test_dataset['timestamp'].min() - validation_dataset['timestamp'].max())/3600/24\n",
    "train_val_gap = (validation_dataset['timestamp'].min() - train_dataset['timestamp'].max())/3600/24\n",
    "print(f\"gap between train dataset and validation dataset: {train_val_gap:.2f} days\")\n",
    "print(f\"gap between validation dataset and test_dataset: {test_val_gap:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dropna(axis=0, subset=\"Target\", inplace=True)\n",
    "validation_dataset.dropna(axis=0, subset=\"Target\", inplace=True)\n",
    "test_dataset.dropna(axis=0, subset=\"Target\", inplace=True)\n",
    "\n",
    "train_dataset.reset_index(inplace=True)\n",
    "validation_dataset.reset_index(inplace=True)\n",
    "test_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Binance Coin',\n",
       " 'Bitcoin',\n",
       " 'Bitcoin Cash',\n",
       " 'Cardano',\n",
       " 'Dogecoin',\n",
       " 'EOS.IO',\n",
       " 'Ethereum',\n",
       " 'Ethereum Classic',\n",
       " 'IOTA',\n",
       " 'Litecoin',\n",
       " 'Maker',\n",
       " 'Monero',\n",
       " 'Stellar',\n",
       " 'TRON']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_names = [asset_list[asset_list['Asset_ID']==aid]['Asset_Name'].item() for aid in range(14)]\n",
    "asset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Asset_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>Bitcoin Cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>Binance Coin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6.779922</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>EOS.IO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>Ethereum Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5.894403</td>\n",
       "      <td>Ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>Litecoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>Monero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>TRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>Stellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>Cardano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>IOTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>3.555348</td>\n",
       "      <td>Dogecoin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Asset_ID    Weight        Asset_Name\n",
       "0          2  2.397895      Bitcoin Cash\n",
       "1          0  4.304065      Binance Coin\n",
       "2          1  6.779922           Bitcoin\n",
       "3          5  1.386294            EOS.IO\n",
       "4          7  2.079442  Ethereum Classic\n",
       "5          6  5.894403          Ethereum\n",
       "6          9  2.397895          Litecoin\n",
       "7         11  1.609438            Monero\n",
       "8         13  1.791759              TRON\n",
       "9         12  2.079442           Stellar\n",
       "10         3  4.406719           Cardano\n",
       "11         8  1.098612              IOTA\n",
       "12        10  1.098612             Maker\n",
       "13         4  3.555348          Dogecoin"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_asset(asset):\n",
    "    asset.sort_values(by='timestamp', inplace=True)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    asset.loc[np.isinf(asset['VWAP']), 'VWAP'] = asset[~np.isinf(asset['VWAP'])]['VWAP'].mean()\n",
    "    asset.loc[np.isnan(asset['VWAP']), 'VWAP'] = asset[~np.isnan(asset['VWAP'])]['VWAP'].mean()\n",
    "\n",
    "    asset['Vol'] = (asset['Close'].diff()/asset['Close'])**2\n",
    "    # add some time series feature\n",
    "    primary_features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\n",
    "    asset[primary_features] = scaler.fit_transform(asset[primary_features])\n",
    "\n",
    "    asset['upper_shadow'] = asset['High'] - asset[['Open', 'Close']].max(axis=1)\n",
    "    asset['lower_shadow'] = asset[['Open', 'Close']].min(axis=1) - asset['Low']\n",
    "    asset['upper_ratio'] = asset['upper_shadow']/(asset['High'] - asset['Low'])\n",
    "    asset['lower_ratio'] = asset['lower_shadow']/(asset['High'] - asset['Low'])\n",
    "    asset['Average_volume'] = asset['Volume']/asset['Count']\n",
    "    asset['Regret'] = asset['VWAP']/asset['Close']\n",
    "\n",
    "    asset['MA5'] = asset['Close'].rolling(5).mean()\n",
    "    asset['MA15'] = asset['Close'].rolling(15).mean()\n",
    "    asset['MA30'] = asset['Close'].rolling(30).mean()\n",
    "    asset['MA60'] = asset['Close'].rolling(60).mean()\n",
    "\n",
    "    asset['VMA5'] = asset['Volume'].rolling(5).mean()\n",
    "    asset['VMA15'] = asset['Volume'].rolling(15).mean()\n",
    "    asset['VMA30'] = asset['Volume'].rolling(30).mean()\n",
    "    asset['VMA60'] = asset['Volume'].rolling(60).mean()\n",
    "\n",
    "    asset['RV5'] = asset['Vol'].rolling(5).sum()\n",
    "    asset['RV15'] = asset['Vol'].rolling(15).sum()\n",
    "    asset['RV30'] = asset['Vol'].rolling(30).sum()\n",
    "    asset['RV60'] = asset['Vol'].rolling(60).sum()\n",
    "    \n",
    "    asset.fillna(0, inplace=True)\n",
    "    \n",
    "    secondary_feature = ['Vol', 'upper_shadow', 'lower_shadow', 'upper_ratio', 'lower_ratio', \n",
    "                         'Average_volume', 'Regret', 'MA5', 'MA15', 'MA30', 'MA60', 'VMA5', \n",
    "                         'VMA15', 'VMA30', 'VMA60', 'RV5', 'RV15', 'RV30', 'RV60']\n",
    "    \n",
    "    asset[secondary_feature] = scaler.fit_transform(asset[secondary_feature])\n",
    "    \n",
    "    return asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_series_feature(data, verbose=False):\n",
    "    processed_assets = []\n",
    "    for aid in range(14):\n",
    "        if verbose:\n",
    "            print(f\"Processing: {asset_names[aid]}\")\n",
    "        asset = data[data['Asset_ID'] == aid].copy(deep=True)\n",
    "        asset = process_one_asset(asset)\n",
    "        processed_assets.append(asset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Concat processed single asset data\")\n",
    "    processed_assets = pd.concat(processed_assets)\n",
    "    processed_assets.sort_values(by='index', inplace=True)\n",
    "    processed_assets.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"One-Hot encoding\")\n",
    "\n",
    "    onehot = pd.get_dummies(processed_assets['Asset_ID'], dtype=int)\n",
    "    for i in range(14):\n",
    "        if i not in onehot.columns:\n",
    "            onehot[i] = 0\n",
    "    onehot = onehot.reindex(columns=list(range(14)))\n",
    "    onehot.columns = [asset_names[aid] for aid in onehot.columns]\n",
    "    processed_assets = pd.concat([processed_assets, onehot], axis=1)\n",
    "    \n",
    "    return processed_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "process training data\n",
      "------------------------------------\n",
      "Processing: Binance Coin\n",
      "Processing: Bitcoin\n",
      "Processing: Bitcoin Cash\n",
      "Processing: Cardano\n",
      "Processing: Dogecoin\n",
      "Processing: EOS.IO\n",
      "Processing: Ethereum\n",
      "Processing: Ethereum Classic\n",
      "Processing: IOTA\n",
      "Processing: Litecoin\n",
      "Processing: Maker\n",
      "Processing: Monero\n",
      "Processing: Stellar\n",
      "Processing: TRON\n",
      "Concat processed single asset data\n",
      "One-Hot encoding\n",
      "------------------------------------\n",
      "process validation data\n",
      "------------------------------------\n",
      "Processing: Binance Coin\n",
      "Processing: Bitcoin\n",
      "Processing: Bitcoin Cash\n",
      "Processing: Cardano\n",
      "Processing: Dogecoin\n",
      "Processing: EOS.IO\n",
      "Processing: Ethereum\n",
      "Processing: Ethereum Classic\n",
      "Processing: IOTA\n",
      "Processing: Litecoin\n",
      "Processing: Maker\n",
      "Processing: Monero\n",
      "Processing: Stellar\n",
      "Processing: TRON\n",
      "Concat processed single asset data\n",
      "One-Hot encoding\n",
      "------------------------------------\n",
      "process test data\n",
      "------------------------------------\n",
      "Processing: Binance Coin\n",
      "Processing: Bitcoin\n",
      "Processing: Bitcoin Cash\n",
      "Processing: Cardano\n",
      "Processing: Dogecoin\n",
      "Processing: EOS.IO\n",
      "Processing: Ethereum\n",
      "Processing: Ethereum Classic\n",
      "Processing: IOTA\n",
      "Processing: Litecoin\n",
      "Processing: Maker\n",
      "Processing: Monero\n",
      "Processing: Stellar\n",
      "Processing: TRON\n",
      "Concat processed single asset data\n",
      "One-Hot encoding\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------\\nprocess training data\\n------------------------------------\")\n",
    "processed_train = process_time_series_feature(train_dataset, verbose=True)\n",
    "del train_dataset\n",
    "print(\"------------------------------------\\nprocess validation data\\n------------------------------------\")\n",
    "processed_val = process_time_series_feature(validation_dataset, verbose=True)\n",
    "del validation_dataset\n",
    "print(\"------------------------------------\\nprocess test data\\n------------------------------------\")\n",
    "processed_test = process_time_series_feature(test_dataset, verbose=True)\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.groups = df.groupby(['timestamp'])\n",
    "        self.timestamps = df['timestamp'].unique()\n",
    "        self.feature_cols = ['Count', 'Open', 'High', 'Low', 'Close','Volume', 'VWAP', 'Vol', \n",
    "                             'upper_shadow', 'lower_shadow', 'upper_ratio', 'lower_ratio', \n",
    "                             'Average_volume', 'Regret', 'MA5', 'MA15', 'MA30', 'MA60', 'VMA5', \n",
    "                             'VMA15', 'VMA30', 'VMA60', 'RV5', 'RV15', 'RV30', 'RV60', 'Binance Coin', \n",
    "                             'Bitcoin', 'Bitcoin Cash', 'Cardano', 'Dogecoin', 'EOS.IO', 'Ethereum', \n",
    "                             'Ethereum Classic', 'IOTA', 'Litecoin', 'Maker', 'Monero', 'Stellar', 'TRON']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.timestamps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ts = self.timestamps[idx]\n",
    "        data = self.groups.get_group(ts)\n",
    "        x = data.loc[:, self.feature_cols].values\n",
    "        num_assets = x.shape[0]\n",
    "        if num_assets < 14:\n",
    "            padding = np.zeros((14-num_assets, x.shape[1]))\n",
    "            x = np.concatenate((x, padding), axis=0)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        y = data.loc[:, 'Target'].values\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        y = F.pad(y, (0, 14-num_assets))\n",
    "        return x, y, num_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoTransformer(nn.Module):\n",
    "    def __init__(self, model_dim=64, num_heads=2, num_layers=2, dropout=0.2, ffn_dim=128, num_features=40):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_up = nn.Linear(num_features, model_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = model_dim, \n",
    "                                                        nhead = num_heads,\n",
    "                                                        dim_feedforward = ffn_dim,\n",
    "                                                        dropout = dropout,\n",
    "                                                        batch_first = True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(model_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.input_up(x)\n",
    "        enc_x = self.encoder(emb_x)\n",
    "        output = self.output(enc_x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer, device):\n",
    "    running_loss = 0\n",
    "    current = 0\n",
    "    net.train()\n",
    "\n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y, num_assets) in enumerate(t):\n",
    "            batch_size = X.shape[0]\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            num_assets = num_assets.to(device)\n",
    "            y_pred = net(X)\n",
    "            y_pred = y_pred.view(batch_size, 14)\n",
    "\n",
    "            mask = torch.arange(14).expand(batch_size, 14).to(device)\n",
    "            mask = mask < num_assets.unsqueeze(1)\n",
    "            y_pred = y_pred * mask.float()\n",
    "            y = y * mask.float()\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (batch_size * loss.item() + running_loss * current) / (batch_size + current)\n",
    "            current += batch_size\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn, device):\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y, num_assets) in enumerate(t):\n",
    "                batch_size = X.shape[0]\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                num_assets = num_assets.to(device)\n",
    "                y_pred = net(X)\n",
    "                y_pred = y_pred.view(batch_size, 14)\n",
    "\n",
    "                mask = torch.arange(14).expand(batch_size, 14).to(device)\n",
    "                mask = mask < num_assets.unsqueeze(1)\n",
    "                y_pred = y_pred * mask.float()\n",
    "                y = y * mask.float()\n",
    "\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                running_loss = (batch_size * loss.item() + running_loss * current) / (batch_size + current)\n",
    "                t.set_postfix({'running_loss':running_loss})\n",
    "                \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./checkpoint'):\n",
    "    os.mkdir('./checkpoint')\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 40\n",
    "MODEL_DIM = 32\n",
    "FFN_DIM = 64\n",
    "DROPOUT = 0.3\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "MAX_EPOCH = 10\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(processed_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(processed_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 27K, number of trainable parameters: 27K\n"
     ]
    }
   ],
   "source": [
    "net = CryptoTransformer(model_dim=MODEL_DIM, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, dropout=DROPOUT, ffn_dim=FFN_DIM).to(device)\n",
    "net.apply(init_weights)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.AdamW(net.parameters())\n",
    "total_num = sum(p.numel() for p in net.parameters())\n",
    "trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_num/1e3:.0f}K, number of trainable parameters: {trainable_num/1e3:.0f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1166/14609 [01:44<20:02, 11.18it/s, running_loss=0.0384]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb 单元格 22\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, MAX_EPOCH\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_loop(train_dataloader, net, loss_fn, optimizer, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m val_loop(val_dataloader, net, loss_fn, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     tb\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m\"\u001b[39m, train_loss, t)\n",
      "\u001b[1;32m/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb 单元格 22\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(dataloader) \u001b[39mas\u001b[39;00m t:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch, (X, y, num_assets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(t):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         batch_size \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb 单元格 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestamps[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mget_group(ts)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mloc[:, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_cols]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m num_assets \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luoxinyang/Desktop/HKUST/2023Fall/MAFS6010Z/project3.nosync/train.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m num_assets \u001b[39m<\u001b[39m \u001b[39m14\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:1339\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:994\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[1;32m    992\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m    995\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1384\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1321\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1517\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1518\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1520\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1522\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:6109\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6106\u001b[0m     keyarr \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6109\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer_for(keyarr)\n\u001b[1;32m   6110\u001b[0m     keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(keyarr)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:6096\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6078\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6079\u001b[0m \u001b[39mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6080\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6093\u001b[0m \u001b[39marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6094\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6095\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6096\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(target)\n\u001b[1;32m   6097\u001b[0m indexer, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6098\u001b[0m \u001b[39mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:3869\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3867\u001b[0m method \u001b[39m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3868\u001b[0m orig_target \u001b[39m=\u001b[39m target\n\u001b[0;32m-> 3869\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_cast_listlike_indexer(target)\n\u001b[1;32m   3871\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3873\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:6622\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_cast_listlike_indexer\u001b[39m(\u001b[39mself\u001b[39m, target) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m   6619\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6620\u001b[0m \u001b[39m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6622\u001b[0m     \u001b[39mreturn\u001b[39;00m ensure_index(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:7569\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7567\u001b[0m         \u001b[39mreturn\u001b[39;00m Index(index_like, copy\u001b[39m=\u001b[39mcopy, tupleize_cols\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   7568\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 7569\u001b[0m     \u001b[39mreturn\u001b[39;00m Index(index_like, copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/indexes/base.py:559\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    556\u001b[0m         data \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(data, dtype\u001b[39m=\u001b[39m_dtype_obj)\n\u001b[1;32m    558\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     arr \u001b[39m=\u001b[39m sanitize_array(data, \u001b[39mNone\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    560\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mindex must be specified when data is not list-like\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gresearch/lib/python3.9/site-packages/pandas/core/construction.py:544\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39mSanitize input data to an ndarray or ExtensionArray, copy if specified,\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mcoerce to the dtype if specified.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mnp.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m original_dtype \u001b[39m=\u001b[39m dtype\n\u001b[0;32m--> 544\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39;49mMaskedArray):\n\u001b[1;32m    545\u001b[0m     data \u001b[39m=\u001b[39m sanitize_masked_array(data)\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, NumpyEADtype):\n\u001b[1;32m    548\u001b[0m     \u001b[39m# Avoid ending up with a NumpyExtensionArray\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_val_loss = float('inf')\n",
    "best_epoch = 1\n",
    "\n",
    "tb = SummaryWriter(log_dir='./logs/experiment2/')\n",
    "\n",
    "for t in range(1, MAX_EPOCH+1):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer, device)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn, device)\n",
    "    tb.add_scalar(\"Train Loss\", train_loss, t)\n",
    "    tb.add_scalar(\"Val Loss\", val_loss, t)\n",
    "    \n",
    "    torch.save(net, f\"./checkpoint/experiment2/epoch_{t}.pt\")\n",
    "    if val_loss < min_val_loss:\n",
    "        best_epoch = t\n",
    "        min_val_loss = val_loss\n",
    "print(f\"best epoch: {best_epoch}, minimun validations loss: {min_val_loss:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('./logs/experiment1/epoch_1.pt')\n",
    "test_dataset = MyDataset(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81010 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81010/81010 [01:02<00:00, 1288.12it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "net.eval()\n",
    "net = net.cpu()\n",
    "with torch.no_grad():\n",
    "    with tqdm(test_dataset) as t:\n",
    "        for x, y, num_assets in t:\n",
    "            y_pred = net(x)\n",
    "            y_pred = y_pred[:num_assets]\n",
    "            predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.cat(predictions)\n",
    "predictions = predictions.numpy()\n",
    "processed_test['Predictions'] = predictions\n",
    "asset_list.set_index('Asset_ID', inplace=True)\n",
    "processed_test['Weight'] = processed_test['Asset_ID'].apply(lambda x: asset_list.loc[x]['Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(x, w):\n",
    "    return np.sum(x*w)/np.sum(w)\n",
    "\n",
    "def weighted_cov(x, y, w):\n",
    "    mean_x = weighted_mean(x, w)\n",
    "    mean_y = weighted_mean(y, w)\n",
    "    return weighted_mean((x-mean_x)*(y-mean_y), w)\n",
    "\n",
    "def weighted_corr(x, y, w):\n",
    "    return weighted_cov(x, y, w)/np.sqrt(weighted_cov(x, x, w) * weighted_cov(y, y, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008556166236553724"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = weighted_corr(processed_test['Predictions'], processed_test['Target'], processed_test['Weight'])\n",
    "print(f'Weighted Pearson Coefficient: {corr:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
